{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST in Flux - CPU version\n",
    "This notebook trains a simple MLP on F-MNIST without using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, MLDatasets\n",
    "using Flux: onehotbatch, argmax, crossentropy\n",
    "using Flux.Optimise: runall\n",
    "using Flux.Tracker: back!, value, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test x:(784, 60000)\n",
      "Test y:(10, 60000)\n",
      "Validation x:(784, 10000)\n",
      "Validation y:(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "x = FashionMNIST.convert2features(FashionMNIST.traintensor());\n",
    "println(\"Test x:\", size(x))\n",
    "y = onehotbatch(FashionMNIST.trainlabels(), 0:9)\n",
    "println( \"Test y:\",size(y))\n",
    "\n",
    "xt = FashionMNIST.convert2features(FashionMNIST.testtensor());\n",
    "println(\"Validation x:\",size(xt))\n",
    "yt = onehotbatch(FashionMNIST.testlabels(), 0:9);\n",
    "println(\"Validation y:\", size(yt))\n",
    "\n",
    "dataset = Iterators.repeated((x, y), 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MLP model, loss, accuracy and solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(784, 128, relu), \n",
    "    Dense(128, 32, relu),\n",
    "    Dense(32,10),\n",
    "    softmax\n",
    ")\n",
    "\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "\n",
    "accuracy(x, y) = mean(argmax(model(x)) .== argmax(y))\n",
    "\n",
    "opt = ADAM(params(model));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function \n",
    "Flux provides a `train!` function but its built-in support for callbacks is very limited so we define a custom one instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct(x) = string(floor(Int64, 100*x), \"%\")\n",
    "function callback(i, n, l, atrain, atest)\n",
    "    println(\n",
    "        pct(i/n),\n",
    "        \": loss=\", @sprintf(\"%.3f\", l),\n",
    "        \", accuracy on training set=\", pct(atrain),\n",
    "        \", accuracy on test set=\", pct(atest)\n",
    "    )\n",
    "end;\n",
    "\n",
    "function my_train!(loss, data, opt; cb=true)\n",
    "    n = length(data)\n",
    "    for (i, d) in enumerate(data)\n",
    "        l = loss(d...)\n",
    "        isinf(value(l)) && error(\"Loss is Inf\")\n",
    "        isnan(value(l)) && error(\"Loss is NaN\")\n",
    "        back!(l)\n",
    "        opt()\n",
    "        if (cb)&&(i % 10 == 0)\n",
    "            callback(i, n, value(l),accuracy(x, y), accuracy(xt, yt))\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4%: loss=1.484, accuracy on training set=48%, accuracy on test set=48%\n",
      "8%: loss=0.975, accuracy on training set=73%, accuracy on test set=72%\n",
      "12%: loss=0.706, accuracy on training set=77%, accuracy on test set=76%\n",
      "16%: loss=0.597, accuracy on training set=80%, accuracy on test set=79%\n",
      "20%: loss=0.537, accuracy on training set=82%, accuracy on test set=80%\n",
      "24%: loss=0.499, accuracy on training set=83%, accuracy on test set=81%\n",
      "28%: loss=0.470, accuracy on training set=84%, accuracy on test set=82%\n",
      "32%: loss=0.448, accuracy on training set=84%, accuracy on test set=83%\n",
      "36%: loss=0.433, accuracy on training set=85%, accuracy on test set=83%\n",
      "40%: loss=0.416, accuracy on training set=85%, accuracy on test set=84%\n",
      "44%: loss=0.402, accuracy on training set=86%, accuracy on test set=84%\n",
      "48%: loss=0.391, accuracy on training set=86%, accuracy on test set=84%\n",
      "52%: loss=0.380, accuracy on training set=86%, accuracy on test set=85%\n",
      "56%: loss=0.373, accuracy on training set=87%, accuracy on test set=85%\n",
      "60%: loss=0.363, accuracy on training set=87%, accuracy on test set=85%\n",
      "64%: loss=0.356, accuracy on training set=87%, accuracy on test set=85%\n",
      "68%: loss=0.349, accuracy on training set=87%, accuracy on test set=86%\n",
      "72%: loss=0.342, accuracy on training set=88%, accuracy on test set=86%\n",
      "76%: loss=0.336, accuracy on training set=88%, accuracy on test set=86%\n",
      "80%: loss=0.330, accuracy on training set=88%, accuracy on test set=86%\n",
      "84%: loss=0.326, accuracy on training set=88%, accuracy on test set=86%\n",
      "88%: loss=0.320, accuracy on training set=88%, accuracy on test set=86%\n",
      "92%: loss=0.315, accuracy on training set=88%, accuracy on test set=86%\n",
      "96%: loss=0.309, accuracy on training set=89%, accuracy on test set=86%\n",
      "100%: loss=0.307, accuracy on training set=89%, accuracy on test set=86%\n",
      "258.687204 seconds (44.63 M allocations: 245.482 GiB, 27.56% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time my_train!(loss, dataset, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
